{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-05 Reinforcement Learning\n",
    "\n",
    "so far focused on learners that provide forecast price changes, then buy or sell with predicted price change\n",
    "this strategy lacks direction, doesn't tell us when to exit position, etc..\n",
    "\n",
    "### Reinforcement learning provides policies that direct specific actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RL problem\n",
    "\n",
    "Robot interacts with environment (T)\n",
    "Robet takes in state S and outputs an Action (A)\n",
    "\n",
    "Circular process - action acts on T and results in new state, robot examines state and comes with action, etc...\n",
    "\n",
    "How do we find the policy (or ${ \\pi }$) that determines action (${ \\pi }$ or policy can be something simple like a lookup table)\n",
    "\n",
    "\n",
    "\n",
    "There's also the reward (R) - objective is take actions that maximize that reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping trading to RL\n",
    "\n",
    "Enviornment -> Market\n",
    "State -> Features | Holding | Prices | Fundamental Data | etc...\n",
    "Actions -> Buy | Sell | Do Nothing\n",
    "Reward -> How we accrue money based on actions we take\n",
    "\n",
    "\n",
    "### Markov decision problem\n",
    "\n",
    "- Set of states (S) \n",
    "- Set of actions A\n",
    "- Transition function T[s,a,s'] -> in state s, take action a, result in state s'\n",
    "- Reward function R[s,a] -> in particular state s, take action a, then we get reward r\n",
    "\n",
    "Find policy ${ \\pi^* }$ that will maximize reward\n",
    "\n",
    "- policy iteration\n",
    "- value iteration\n",
    "\n",
    "### Unknown transitions and rewards\n",
    "\n",
    "Most of the time we don't have this transition function or this reward function\n",
    "\n",
    "Robot has to interact with world and build that data\n",
    "\n",
    "e.g., this would be an experience tuple (in state s, take action a, get state s', and get reward r):\n",
    "<s, a, s', r>\n",
    "\n",
    "s' = s2\n",
    "<s2, a2, s2', r2>\n",
    "...\n",
    "\n",
    "do this over and over again, we get this trail of experience tuples\n",
    "\n",
    "- model based -> build a model of T[s, a, s'] and R[s,a], build a tabular representation of all these states and results states\n",
    "- model free (Q-Learning), build a policy by looking directly at the data\n",
    "\n",
    "\n",
    "### What to optimize?\n",
    "\n",
    "get 1 dollar a year for a million years?  Or get one million dollars in 3 years after losing 1 for two years\n",
    "\n",
    "- infinite horizon (both strategies repeated over and over will result in infinite rewards)\n",
    "- finite horizon (if we optimize over 3 years, second strat is best strategy)\n",
    "- discounted reward (similar to future rewards, we multiply rewards by gamma so that future rewards aren't worth as much)\n",
    "\n",
    "Q-Learning uses discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
